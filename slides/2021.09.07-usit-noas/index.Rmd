---
title: "A different kind of data base"
subtitle: "- creating a custom data storage, management and user export tool for a research lab -"
author: "Athanasia Monika Mowinckel"
date: "07.09.2021"
output:
  xaringan::moon_reader:
    css: 
      - default
      - lcbc-uio.css
      - lcbc-uio-fonts.css
      - "https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css"
      - "https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
    lib_dir: libs
    nature:
      titleSlideClass: [middle, right]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false 
      ratio: "16:9"
image: img/noas_with_software_trimmed.png
tags:
  - data management
  - dev
---

```{r include = FALSE}
# devtools::install_github("dill/emoGG")
library(tidyverse)
library(emoGG)
library(gganimate)
knitr::opts_chunk$set(
  echo = FALSE,
  out.width = "100%",
  fig.retina = 3
)
```


background-image: url("https://drmowinckels.io/about/profile.png")
background-position: right bottom
background-size: auto 100%
class: middle, dark

.pull-left[
## Athanasia Monika Mowinckel

[<i class="fa fa-twitter fa-2x" aria-hidden="true"></i> @DrMowinckels](https://twitter.com/DrMowinckels)   
[<i class="fa fa-github fa-2x" aria-hidden="true"></i> Athanasiamo](https://github.com/Athanasiamo)  
[<i class="fa fa-globe fa-2x" aria-hidden="true"></i> drmowinckels.io/](https://drmowinckels.io/)  

- Staff scientist / Research Software Engineer  
- PhD in cognitive psychology  
- Software Carpentry Instructor   
- R-Ladies Oslo & [R-Ladies Global team](www.rladies.org)
] 

<!-- --- -->

<!-- layout: true -->

<!-- <div class="my-sidebar"></div>  -->

---
background-image: linear-gradient(to left,#00000074, #00000074 97%), url("https://images.unsplash.com/photo-1544383835-bda2bc66a55d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1321&q=80")
background-size: cover
class: inverse, center, middle

## A story about data

???
I will start with a little background story for this presentation. 
To give you a true understanding of how even data management can evolve over time.
When I finished my PhD, I had been working at LCBC for about 6 months as a project manager. 
LCBC is an exciting center, that collects amazing data, and has been doing so for over 10 years. 

---
background-image: url("lcbc_logo_tall.png")
background-size: 32%
background-position: 10%


.pull-left[]
.pull-right[
```{r aniopts=c("loop = 0")}
tibble(
  Time = c(1:5, 1, 3, 5, 2, 4:5),
  i = c(rep("subject 1", 5), 
        rep("subject 2", 3), 
        rep("subject 3", 3))
) %>% 
  mutate(y = i) %>% 
  ggplot(aes(Time, y, group = i)) +
  geom_emoji(emoji = "1f464", size = .1) +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        axis.title.y = element_blank(),
        axis.text = element_text(size = 30)) +
  transition_time(Time) +
  shadow_mark()

```

]

???
Their data is longitudinal, meaning they follow the same people over and over again as time progresses, and they have SO much data on each. 
MRI, cognitive tests scores, questionnaires, physical measurements, so much!
But, as most centers, they didn't necessarily start out with the means to have such longitudinal studies as they have now.
This means that the data have become longitudinal, but were not planned to be. I emphasise this because it has relevance to the state of the data. 

---
background-image: url("https://images.unsplash.com/photo-1524311583145-d5593bd3502a?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1351&q=80")
background-size: cover
background-position: bottom

???
So, as years went by, they increased the number of individual studies, and each of these studies also started collecting data on both old and new participants. 
And more PhD students and Post-docs came along and worked on the various projects.

Now, there was a lot of borrowing of data between projects. Some projects only collected data from kids, but then someone in another project needed some kids as controls for a hypothesis. Some had patients that needed control subjects etc. So data was sent between projects at times to answer new and exciting questions. 

---
background-image: url("https://images.unsplash.com/photo-1444459094717-a39f1e3e0903?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=800&q=80")
background-position: right
background-size: auto 100%


???
But one of the key problems were that each project had their own coding system, mostly depending on the PhD or Postdoc in charge of the current data collection, and even each round of data collection did not necessarily adhere to the last round.

--

.pull-left[
Different file types:
- text  
- SPSS  
- excel

Different coding systems
- Sex: 
   - 0/1 
   - 1/0
   - 1/2
   - M/F
   - m/f
   - m/k
   - Male/Female
]


???
For instance, when we started with the data harmonisation, I had at least three main types of data to work with, and a single binary variable like "Sex" I found to be coded in no less than 7 different ways.

This is _not_ a jab at my colleagues, this is what naturally happens when projects evolve over time and across people. Data management and naming is a field of its own, and deciding on a single unifying coding scheme across 7 projects with up to 6 rounds of data collection in the different projects over 10 years is no laughing matter. 

This meant that data needed to recoded every time someone wanted to combine data across projects or even across data collection rounds. 

---
background-image: url("https://images.unsplash.com/photo-1537202108838-e7072bad1927?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=685&q=80")
background-size: cover
class: center, bottom, inverse

???
So, every time projects pooled data, harmonisation of the data had to be done. 
Harmonisation means making the data coding system equal (or as equal as possible) across datasets.

--

# Mother of all Spreadsheets

???
After some years, Anders and Kristine, the center leaders realised how much time was being spent in re-harmonising all the data, that they hired someone to start the process of harmonising everything, and create what they called "The Mother of all Spreadsheets". 

--

(MOAS for short)



---
background-image: url("https://images.unsplash.com/photo-1532009877282-3340270e0529?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80")
background-size: cover

???
So the process started, and harmonisation was well under way when I came along. A year into the harmonisation, I took over the job. Now, what was previously somewhere in the magnitude of 100 SPSS files, were being merged using several huge pieces of SPSS syntax. 

Ok. So everyone who has worked with SPSS and has data management experience knows that SPSS is not ideal for big operations like this. The syntax did the basics, but it was error prone, hard to capture the errors, and exceedingly slow. Not to mention the output, which was an SPSS file over 2000 rows and 800 columns that was absolutely huge, in the gigabytes.

---
background-image: url("https://images.unsplash.com/photo-1591786409812-8260a746fd1c?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=917&q=80")
background-size: contain
background-position: top
background-color: lightgrey

???
 SPSS didn't even like to open the file. 

We needed to alter tactics. I came in with experience from bash, matlab and R, and R being my chosen language, started refractoring the code into R and getting something more streamlined and processing-wise more light-weight working.
 I was convinced at the time, that R would enable me to make a robust, non-breakable pipeline for data harmonisation. 
 I would do it once, and it would run perfectly every time after.

After about another month, we had our MOAS working, through R, saving to different file formats and things seemed so amazing!
It was working, it was updating data on a regular basis, it was spitting out files in different formats and in special folders for different purposes. I was sure this was it, life would be easier from now on.

---
background-image: url("https://images.unsplash.com/photo-1552929859-61df5f9622a3?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=676&q=80")
background-size: cover

???
But, everyone with experience in R will also know that R workflows break, easily, if you don't know how to create stable workflows in R.
And I didn't at the time, and indeed at that time, the tools needed to make it truly stable didn't really exist yet.
R updates and packages update and someone touches a file they shouldn't and my workflow was busted. 
And while the workflow was better than SPSS, it was still a monster.

I probably refractored the MOAS workflow 6 or 7 times over a couple of years. Making R-packages to make sure that the flow stayed as equal as possible, but the problems kept growing. There was just too much data, from too many sources, too complex of a pipeline, and too much more data I knew people wanted added. At the end of its life, the MOAS contained over 4000 rows of data, and 2 thousand columns. 
And this was the curated small version... requests to add more data columns were being denied by me on an almost weekly basis.


---
background-image: url("https://images.unsplash.com/photo-1589807502973-c7be2ee95d99?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=634&q=80")
background-size: auto 100%
background-position: left

.pull-left[]


???
Now, the MOAS was doing its job. The data were harmonised, and people were beginning to work with the harmonised data and papers pooling data across projects were off to a better start and were being initiated much more often. But there were huge issues.

--

.pull-right[

## Noone else:

- could update the data  
- could add new data  
- fix the pipeline when broken  
- knew the entire content of the file  

## I was making special

- R-packages to work with the file  
- enormous documentation noone could possibly read
- scripts for people to reduce to the data they needed and merge with other data sources
]

???

I can here mention, that I was just about the only person at the whole center that understood the content of this file and how to work with it. I was the only one who could add or edit the data, and it could take me anywhere to 1 day to a week to get the pipeline running depending on where it broke. 

I was not enabling my colleagues to work independently. I was not enabling my center as a whole to function successfully. It harmed everyone and delayed everyone. It had to die. 


---
background-image: url("https://images.unsplash.com/photo-1535551951406-a19828b0a76b?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1346&q=80")
background-size: cover

???
By this point, we had employed a fulltime programmer. 
This looks nothing like Florian Krull, but he is not one for getting his face plastered on the internet so here is an unsplash photo instead. 
For a center where the engineers actually are psychologists with special interest and aptitude in programming, this was a very welcome addition. 
We finally had someone that could aid us in getting the MOAS ported to an SQL database, something we had been wanting to do for a while, but didn't have the resources to do so. 

Inge and I had actually initiated the port some years before, with help from USIT, but we found our time always being captured by other pressing projects, and the port was put on hold.

---
background-image: url("https://images.unsplash.com/photo-1535551951406-a19828b0a76b?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1346&q=80"), url("https://pbs.twimg.com/profile_images/1120988795338481666/OE4M65CS_400x400.png"),url("https://drmowinckels.io/about/profile.png")
background-size: 40%, 30%, 30%
background-position: bottom, left top, right top
class: center


???
So Inge, Florian, and I sat down and started discussing how to go about this.
What does the data look like? what do we need from the storage solution? how should people access the data? how do we document the data?

--

Data needs to be traceable  

--

Data needs to be searchable  

--

Data needs to be accessible through  
an UI and programatically  

--

Data documentation must be  
easily available


???
The benefit we had as a team, was that Florian could come in with a very structured way of working and thinking about developing tools. While Inge has his foot in both doors, having IT background and being a research psychologist, understanding systems and how scientists work. I, having no formal IT training, had full overview of the data structures and content and understood the types of hurdles people had been facing with the old MOAS.

There were many decisions to be made, and at the end we decided the best was to start something, and then build it bit by bit.


---
background-image: url("https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2021/05/postgresql-database-logo-1.jpeg")


???
So, we were developing the actual system that would hold all this information. We knew we wanted to use SQL, as we have this available on TSD, where our data need to live. 
A proper relational database would create a data system we could access in many different ways.

But we also knew that SQL could not keep track of the edits we made to data in the way we wanted.
We wanted something that could do this easily and natively, rather than trying to set up separate logging systems. Everyone knows manual log systems fall apart at some point. 

So how would be both have a database, and keep track of all data changes?

---
background-image: url("https://chilli.codes/wp-content/uploads/2020/10/git.jpg")


???
We looked to git. We opted for having all our data in tab separated text files, tracked by git version control, and create a system that would import the data to SQL upon request, and make the database available to users. 

This means that every time we add or edit data, we would prompt the SQL database to repopulate it self with the new data, and let git do all the work of keeping track of data changes.
It also means we have unique git fingerprints for each database state without extra effort, which is really nice for keeping track of ongoing manuscripts.

This means that, differently from many other databases, our SQL database is disposable, as the source data is stored as text files.

---
background-image: url("https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/1280px-R_logo.svg.png"), url("https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Postgresql_elephant.svg/1200px-Postgresql_elephant.svg.png")
background-size: 40%, 30%
background-position: 10%, 80%
class: middle, center 

<h2 style="font-size: 100px;"> + </h2>

???
For the import to the database, we use R to create SQL calls to import the data. 
This gives us some control over the entire process, having specialised templates for each of the data categories we have. 
The import is fairly verbose, letting us know when something errors, and what has been imported successfully.
There are also instances were data is not imported, but only warnings are thrown. 
In such cases the data tables contain data that is not represented in the core tables of the relational database, and therefore they cannot be added.
This is a way for us to know when new core entries might be needed. 

---
class: dark, middle, center
background-image: url("https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg"), url("https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/1280px-R_logo.svg.png"), url("https://github.com/tidyverse/dplyr/blob/master/man/figures/logo.png?raw=true"), url("https://github.com/tidyverse/tidyr/blob/master/man/figures/logo.png?raw=true"), url("https://github.com/tidyverse/purrr/blob/master/man/figures/logo.png?raw=true"), url("https://flask-training-courses.uk/images/flask-logo.png")
background-size: 20%, 20%, 10%, 10%, 10%, 15%
background-position: 10% 30%, 88% 30%, 90% 70%, 79% 70%, 84.5% 87%, 10% 85%

## Work with what you know

???
In the beginning, we were focused on getting something working as fast as possible. Feel a proper progress right away.
We were unfamiliar with SQL, so learning how to work with that was taking enough time, we wanted the remaining development to go forward quickly. 
So we were working with R with heavy dependencies, and python with flask for creating a UI. 

This got us started using mock data for local development outside TSD. And once we had something working that we thought was doing the job, we started stripping dependencies.

---
background-image: url("https://www.uio.no/english/services/it/research/sensitive-data/pictures/tsd_logo-test-liten.png")
background-size: 25%

???
As I mentioned early on, our data needs to live on TSD. The amount of data we have of people, and over time, means that our data is quite sensitive when gathered together. 

This is something we needed to keep in mind when developing. We needed a system that we could run on TSD, without it breaking on TSD when TSD software was changed. This meant setting up ways and using software we could bundle and compile our selves, rather than relying directly on TSD infrastructure.

TSD staff already get a lot of tickets, and we didn't want our data solution to be prone to errors we our selves could not mend. This meant that our first approach, while working, could not really work on TSD because of the massive dependencies we layered on it. Refractoring the code to be portable became key priority.

---
class: dark

## Minimizing dependencies and bundling software

???
This meant quite some work on the part of Florian and I, who were at this point the two mainly responsible for the source code of the entire project. 

--

.pull-left[
R package dependencies:
 - [RPostgreSQL](https://cran.r-project.org/web/packages/RPostgreSQL/index.html)  
 - [digest](https://cran.r-project.org/web/packages/digest/index.html)  
 - [jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html)  
 - [DBI](https://cran.r-project.org/web/packages/DBI/index.html)
]

???
I was stripping R dependencies as much as I could, relying only on light packages that needed no special installs (like gcc). And Florian was bumping python and flask for php, and creating the UI with HTML, CSS and javascript.

--

.pull-right[

UI dependencies:
- [lighttpd](https://www.lighttpd.net/)  
- php  
- HTML, CSS (bootstrap)  
- javascript  
]


<center>
<br>
<h3>postgresql</h3>
</center>


???

We made sure everything we used we could bundle before porting the system into TSD. Which Florian again made a great effort with through creating Make-files that would download all necessary software, like R-packages, php, postgresql etc, and another make file that would install and setup the system within TSD.

---

layout: true

<div class="my-sidebar"></div>

---
background-image: url("img/noas_structure_trimmed.png")

???
So we end up with a system that roughly looks like this.
At the bottom we have the data, the text-files in a special curated format, split into two distinct sections.
The data are imported to the Database, which make the data available for queries.
Then we have our API layer, This is where we control how the data can be selected and joined.
Since we study people, we have some that consent to their data being used in a single project, while most consent to their data being used across all our projects. 
This means we need to control so that the persons who have not consented do not have their data merged unless the query is for the single project. 
And we have meta-data that document what the variables in the various tables are, to help the researchers navigate the content.
The API is most commonly accessed through the UI we've made, but can also be accessed from programs like R and python given the correct tools and information about the Database connection. 
This gives great flexibility for everyone.

---

## The data

.pull-left[
```sh
core/
  - subjects_proj1.tsv
  - subjects_proj2.tsv
  - projects.tsv
  - waves.tsv
  - visits_proj1_w1.tsv
  - visits_proj1_w2.tsv
  - visits_proj2_w1.tsv
```
]

???
where core data is essential data on participants on projects that need to be in order for the remaining data base to be built.
Core data is strictly monitored and only edited by data managers, while non_core data is experimental data that can be corrected and added to by others (reviewed and approved by data managers). 

--

.pull-right[
```sh
non_core/
  bdi/
    - _metadata.json (optional)
    - _noas.json
    - bdi_proj1_w1.tsv
    - bdi_proj1_w2.tsv
  cvlt/
    - _metadata.json (optional)
    - _noas.json
    - cvlt_proj1_w1.tsv
    - cvlt_proj2_w1.tsv
```
]

???
Subjects and projects are the only two tables with no dependencies in other tables. For an entry to be added to waves, the project that wave belongs to needs to exist in projects. And for any visit of a participant to be added, that participant needs to be present in subjects, and the project and wave must exist.

---
background-image: url("img/noas_relations_1_trimmed.png")
background-size: 85%


???
The relational database looks roughly like this. 
The core data create the basis for the remaining database to be populated, and three main categories of non_core data are linked to the core data in their separate ways to maintain a clear structure.

You think maybe that we spent most our discussions on the software and how we create the system. But actually, while the coding definitely took the longest, the longest discussions we had, and continue to have, is the formatting of the data, and what types of data structures we need. 

---
background-image: url("img/noas_relations_2_trimmed.png")
background-size: 85%


???
And the three different types of non_core data are connected to the core data in differing ways. Cross-sectional data do not need visits, this is static information about persons, like their genes, so they are connected directly to the subjects table.

The longitudinal data are measurements taken at each assessment round of projects, what we call waves. Data collection waves. So these are collected with intervals typically somewhere between 2-5 years, except for some experimental designs where they are assessed every few months.
These are connected to the visits table, i.e. to a subject, specific project and specific data collection wave within that project.

The repeated data encompass things like time-series data from computerised tests, or any variable that is collected several times within a data collection wave. These are also connected to visits, but have a fourth primary key column, which is allowed to differ between repeating tables depending on what is most meaningful. 

All data we have can be fit into these three categories, and the API that we developed knows how to merge data given the type of data category a table is in. 

---
background-image: url("img/noas_relations_3_trimmed.png")
background-size: 85%

???
In time-series tables, this is typically a timestamp for each time series response. In this case, I have MRI data, which for about 200 of our subjects have been collected at multiple scanners within a data collection wave. So the forth column is a string with the name of the scanner used.

---
class: dark

```sh
> make run_dbimport
core/projects.tsv 
core/waves.tsv 
core/subjects_w1.tsv 
core/subjects_w2.tsv 
core/visits_w1.tsv 
core/visits_w2.tsv 
non_core/pgs_scz/pgs_ad.tsv 
WARNING:  missing core data (subject_id=9900006) in file pgs_scz/pgs_ad.tsv
WARNING:  missing core data (subject_id=9901637) in file pgs_ad/pgs_ad.tsv
non_core/pgs_ad/_metadata.json 
non_core/mri_aseg/mri_aseg.tsv 
WARNING:  missing core data (subject_id=9900895, project_id=Proj3, wave_code=1) in file mri_aparc/mri_aparc.tsv
non_core/mri_aparc/_metadata.json 
non_core/ehi/Proj1_01.0.tsv 
WARNING:  missing core data (subject_id=9910992, project_id=Proj1, wave_code=1) in file ehi/Proj1_01.0.tsv
WARNING:  missing core data (subject_id=9910681, project_id=Proj1, wave_code=1) in file ehi/Proj1_01.0.tsv
non_core/ehi/_metadata.json 
non_core/cvlt/Proj2_01.0.tsv 
non_core/cvlt/_metadata.json 
import complete
```

???
Once we have all our data and the necessary jsons for the import have been created, we can start our import. While the import is run in R, it is initiated from command line using a make file, and the make command. The entire process of preparing, initiating and populating the database is run through make files. As well as starting the UI, pulling and implementing source code changes etc.

The import is verbose, and lets us know what data and in what file are not added because there is no corresponding core data to connect it to. Errors are also thrown verbosely, so this log always gives us clear ideas of what we need to fix to get the import running. 

It is also verbose on meta-data import. The meta-data is actually stored in the database as its own little relational database of table information. And one of they key things the meta-data does is alter the data columns to the correct data types.

---

.pull-left[

## Meta-data

`_metadata.json`

- for transforming data
- for documentation
- for easier data searches
]


```json
{
  "title": "Edinburgh Handedness Inventory",
  "category": [
    "questionnaire"
  ],
  "columns": [
    {
      "descr": "nominal score (factor of answer to question 1)",
      "id": "nominal",
      "type": "text"
    },
    {
      "descr": "completion date",
      "id": "date",
      "type": "date"
    },
    {
      "descr": "question 01",
      "id": "01",
      "type": "integer"
    }
}
```

???
All data is initially imported as string, so the import it self happens smoothly. Then, based on information in the meta-data json, there are calls to SQL to convert columns to specified types. This gives us very clear error messages during import, if the import it self is failing, or the conversion from string to another data type is failing.

meta-data is optional, but very much recommended when adding new tables.

---

## Neccessary table information
`_noas.json`

.pull-left[
```json
{
  "table_type": "repeated",
  "repeated_group": "mri"
}
```
]

.pull-right[
- for correct connection to core data
- for correctly joining tables to each other
- for correctly identifying repeated tables belonging together
]

???
In the first versions of the system, we did not have this system file. We organised the three different table types into their own folders, and imported according to which folder they were in.

We found, however, that this system made it possible to have tables with the same name but of different types. This was not an ideal situation, and we opted to have one large non_core folder, with all tables in their own sub-folder. Then we would know there were no table name duplications.

But that meant needing a way to let the system know which table type this was, to check against the core data. So we created the noas json, which has this information. For repeated tables, there is also the optional "repeated_group" specification, which is used when joining data. This indicated that data within this group all have the same content in their fourth column and should be joined by taking this into account.

---
background-image: url("https://images.unsplash.com/photo-1587620962725-abab7fe55159?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1489&q=80")



???
We decided on how tables should look like to fit this new idea, and I started the job of now splitting the MOAS into smaller distinct files for a database. 
Now that data was harmonised, we were in the position of being able to create completely tidy data structures, well suited for a data-base. And a database is so much more convenient than a huge spreadsheet. 

Its more computationally and humanly effective to build a dataset you need, rather that start with the biggest dataset you can and then frantically search for what you need in it. 


---

## Computed or derived data

.pull-left[

```{r, echo = FALSE}
library(ggseg)
library(ggplot2)
ggplot() + 
  geom_brain(atlas = dk, 
             position = position_brain(hemi ~ side),
             show.legend = FALSE) +
  scale_fill_brain2(dk$palette) +
  theme_void()

```
]

???
At LCBC, we routinely work with derived, or computed data. That is data not in its raw form, but aggregated or derived from raw in some way. A common type of data is metrics from what we call brain atlases. That could be measures of cortical thickness or volume within distinct anatomical regions of the brain, these are nicely formatted tabular data that we _could_ have in git.

--

.pull-right[
**Don't version control:**
- they quickly inflate history
- don't get edited by hand
- software updates may make it hard to reproduce 100%
- additions of more data may alter previously derived data

**Solution:**
- keep several versions of the derived data
- git the symlink to the most current one
]

---
background-image: url("https://www.uio.no/english/services/it/research/sensitive-data/pictures/tsd_logo-test-liten.png")
background-size: 25%
background-position: 80%

.pull-left[
## Get it working
- always running
- always trusted
- transparent workflow
- runs checks on data before/during import

Limitations:
- can't expect all to be comfy with command line git
- can't expect all to run local data checks before import
- data administrators need an easy and fast way to approve data changes
]


???
So, we have our data, we have a system, now we need to implement it on TSD, and figure out the workflow of this system. So, having the program is one thing, its another to actually create a system the program works in that creates a stable workflow for everyone.

One where data is always accessible, can be trusted, where multiple users can work together, where we have checks before new or edited data enter the database.

The system needed more to run smoothly, to be practical.

---
background-image: url("img/tsd_gitea_trimmed.png")
background-size: 80%

## Gitea

???
Inside TSD, Florian had already set up Gitea on our service machine. Constantly running, the remote git friend to all of those used to git and GitHub, just inside TSD.
This remote git server runs for our entire project, and we use it actively for shared and own projects. For those of us not 100% comfortable with command line git alone, remote git servers provide a nice way of interacting with git through a UI. 

---
background-image: url("https://upload.wikimedia.org/wikipedia/commons/b/bb/Gitea_Logo.svg")
background-position: 10%

.pull-left[]
.pull-right[
## NOAS data repository

Branches: 
 - **master** 
    - verified current main source data
    
 - **live**   
    - where all edits and new data are added temporarily before managers merge into **master**
]

???
We have two protected branches at all times. 
These are needed to keep the data in continuous good shape.
All changes and adds to data are always first merged into live from forks or other branches, before ultimately being merged into master by a data manager (usually me).

This means we can amass quite some additions or changes in live before we decide on merging into master. The data from both these branches are accessible through API or UI to each separately. So users who desperately need data in live, before a manager can merge them to master, may still access that data. 

But we have a lab policy to never publish on analyses based on live, it can be used for setting up pipelines and preparing for master to be updated, but it has not been scrutinized yet.

Having the data on Gitea means those without familiarity with command line git may also contribute in editing data, though this solution still requires more technical affinity that we can expect from all staff. And some tables are rather huge, and interacting with large tab-separated files on a system that does not create cell grids can be dangerous. 

We are working on a separate javascript based solution towards the Gitea API for editing data for an even easier interface.

---
background-image: url("https://upload.wikimedia.org/wikipedia/commons/e/e9/Jenkins_logo.svg")
background-position: 20%
class: dark


???
Now the last piece in our puzzle. Jenkins.
We needed something that would run automated checks for us on the data. And the best automated check we have, is to literally try importing the data into a database. 

--

.pull-left[]
.pull-right[

## Continuous integration

- live
   - runs on all PRs or pushes/pulls to live
   
- master 
   - runs on PR or pushes/pulls from live
   
- ci
   - runs on all pushes/pulls to other branches
]

???
But data managers should not need to manually pull the data repo, and locally run checks. This is error prone and the lazy of us know we would get sloppy over time. We needed a butler, someone to run continuous integration for us and just run the checks and actually populate the two databases we had, live and master. 

So, Florian got Jenkins running and connected to Gitea, and now we have three CI runners

---
background-image: url("img/noas_entire_trimmed.png")
background-size: 80%

???
So, in the end, we have this system that runs multiple instances of our database simultaneously, allowing us to edit and run automated checks on any new data _before_ they are added to the database. Ensuring that the data are always available (except for the 15 minutes when import is run, when connecting to the database is not available).

---
class: dark, middle, center

## Demo of WebUI

---
background-image: url("img/noas_ark.png")
background-size: 40%
background-position: 90%
class:dark
--

## NOAS
Nephew of all Spreadsheets

--

.pull-left[
- source data in git
- database as disposable
- robust set of software dependencies that can be bundled
- allows multiple types of interface to data

- [source code on GitHub](https://github.com/LCBC-UiO/noas)
]

???
So the system has been active for over a year now, and the data is constantly added to, edited and is getting to a more and more stable state. We get feedback from users and try to solve their wishes as we can, without adding to the complexity of what we already have.

In many instances, these suggestions cannot be catered too, but then we attempt to solve the need by creating new tools that can connect to the Gitea API for their convenience. Our main aim for this specific system is a stable constantly operational database for all staff to use.

---

layout: false

---
background-image: url("https://drmowinckels.io/about/profile.png")
background-position: right bottom
background-size: auto 100%
class: middle, dark

.pull-left[
## Athanasia Monika Mowinckel

[<i class="fa fa-twitter fa-2x" aria-hidden="true"></i> @DrMowinckels](https://twitter.com/DrMowinckels)   
[<i class="fa fa-github fa-2x" aria-hidden="true"></i> Athanasiamo](https://github.com/Athanasiamo)  
[<i class="fa fa-globe fa-2x" aria-hidden="true"></i> drmowinckels.io/](https://drmowinckels.io/)  

- Staff scientist  
- PhD in cognitive psychology  
- Software Carpentry Instructor   
- R-Ladies Oslo & [R-Ladies Global team](www.rladies.org)
] 
